{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#%%capture\n!pip install unsloth # âœ… Installs the Unsloth library, which is designed for super-efficient fine-tuning of large language models (LLMs).\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:36:52.446626Z","iopub.execute_input":"2025-02-14T13:36:52.446969Z","iopub.status.idle":"2025-02-14T13:40:23.226277Z","shell.execute_reply.started":"2025-02-14T13:36:52.446940Z","shell.execute_reply":"2025-02-14T13:40:23.225301Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.2.9-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.2.3 (from unsloth)\n  Downloading unsloth_zoo-2025.2.4-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.14-py3-none-any.whl.metadata (9.4 kB)\nCollecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.2.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\nCollecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<0.15.0,>=0.7.9 (from unsloth)\n  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.28.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.31.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.11)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<0.15.0,>=0.7.9->unsloth) (13.9.4)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.2.3->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.2.3->unsloth) (11.0.0)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->unsloth) (8.5.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<0.15.0,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<0.15.0,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<0.15.0,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nDownloading unsloth-2025.2.9-py3-none-any.whl (185 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m185.6/185.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.2.4-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m965.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.14-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nInstalling collected packages: triton, nvidia-cusparselt-cu12, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 24.12.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 24.12.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.2 cut_cross_entropy-25.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 shtab-1.7.1 torch-2.6.0 torchvision-0.21.0 transformers-4.48.3 triton-3.2.0 trl-0.14.0 tyro-0.9.14 unsloth-2025.2.9 unsloth_zoo-2025.2.4 xformers-0.0.29.post3\nCollecting git+https://github.com/unslothai/unsloth.git\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-30flosrd\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-30flosrd\n  Resolved https://github.com/unslothai/unsloth.git to commit 179840d3a7b49188c372b56c67c4290d53c29ed6\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2025.2.9-py3-none-any.whl size=185189 sha256=b009e39f8fb47740b668d9e1a4c0418cb377315967767de58d794a4e6c5183d1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-1wka0ca8/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\nSuccessfully built unsloth\nInstalling collected packages: unsloth\n  Attempting uninstall: unsloth\n    Found existing installation: unsloth 2025.2.9\n    Uninstalling unsloth-2025.2.9:\n      Successfully uninstalled unsloth-2025.2.9\nSuccessfully installed unsloth-2025.2.9\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom trl import SFTTrainer\nfrom unsloth import is_bfloat16_supported\nfrom huggingface_hub import login \nfrom transformers import TrainingArguments\nfrom datasets import load_dataset\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:44:30.994755Z","iopub.execute_input":"2025-02-14T13:44:30.995111Z","iopub.status.idle":"2025-02-14T13:45:02.589960Z","shell.execute_reply.started":"2025-02-14T13:44:30.995080Z","shell.execute_reply":"2025-02-14T13:45:02.589035Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#from huggingface_hub import login\n#from kaggle_secrets import UserSecretsClient\n#import wandb\n\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)\n\nwb_token = user_secrets.get_secret(\"WANDB\")\nwandb.login(key=wb_token)\n\nrun = wandb.init(\n    project = 'fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:45:15.391255Z","iopub.execute_input":"2025-02-14T13:45:15.391574Z","iopub.status.idle":"2025-02-14T13:45:28.076334Z","shell.execute_reply.started":"2025-02-14T13:45:15.391547Z","shell.execute_reply":"2025-02-14T13:45:28.075716Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaid\u001b[0m (\u001b[33mnikolaid-3cx\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250214_134522-i1hlllxs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/i1hlllxs' target=\"_blank\">thrilling-caress-7</a></strong> to <a href='https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset' target=\"_blank\">https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/i1hlllxs' target=\"_blank\">https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/i1hlllxs</a>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"#load DeepSeek R1 and Tokenizer \n#from unsloth import FastLanguageModel\n#import torch\nmax_seq_length = 2048 #consider increasing this to 4096 or 8192 to have longer contexts \ndtype = None  #bfloat16 to speed up training on nvidia A100/H100\nload_in_4bit = True #Enables 4bit quantization \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype, \n    load_in_4bit = load_in_4bit, \n    token = hf_token,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:45:41.346550Z","iopub.execute_input":"2025-02-14T13:45:41.346921Z","iopub.status.idle":"2025-02-14T13:46:07.132590Z","shell.execute_reply.started":"2025-02-14T13:45:41.346893Z","shell.execute_reply":"2025-02-14T13:46:07.131938Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.9: Fast Llama patching. Transformers: 4.48.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dea2c1954f1b4e32ac0ef333c25bf13f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"879303f660474c15bb071c01628b427c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9702453318a642c6a86a74103b3574cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7f8e8fa15f41839f17573c999ebc6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2797910e1f54c608de4599abb08af93"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#Test R1 on a medical use case before fine tuning \n#Define a system prompt style with placeholders  \n\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:46:29.756252Z","iopub.execute_input":"2025-02-14T13:46:29.756547Z","iopub.status.idle":"2025-02-14T13:46:29.761337Z","shell.execute_reply.started":"2025-02-14T13:46:29.756523Z","shell.execute_reply":"2025-02-14T13:46:29.760427Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Test Deepseek by providing a medical question and generating a response \n# Set a test question \n# Format the question using structured Prompt style, Tokenize the input and  move it to GPU \n# Generate a response \n# Decode the output tokens back into the text to get the final readable response \n\nquestion = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\nFastLanguageModel.for_inference(model) #Unsloth is x 2 faster than huggingface \ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  #tokenized input questions moved to the gpu \n\n# get more param ideas from the cat to tweak the generation settings to improve responses \noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\n\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:51:43.907194Z","iopub.execute_input":"2025-02-14T13:51:43.907494Z","iopub.status.idle":"2025-02-14T13:52:11.170764Z","shell.execute_reply.started":"2025-02-14T13:51:43.907472Z","shell.execute_reply":"2025-02-14T13:52:11.169877Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so I need to figure out what cystometry would show for this 61-year-old woman. Let me break this down step by step.\n\nFirst, the patient has a history of involuntary urine loss, especially when she coughs or sneezes. That makes me think of stress urinary incontinence. I remember that stress incontinence is usually due to the urethral sphincter not closing properly during activities that increase abdominal pressure, like coughing. But she doesn't leak at night, which is more common in other types of incontinence, like nocturnal enuresis, which is typically due to neurogenic causes.\n\nShe underwent a gynecological exam and a Q-tip test. I'm not too familiar with the Q-tip test, but I think it's used to assess urethral function. The Q-tip is a small device used to measure the closure pressure of the urethral sphincter. If the pressure is low, it might indicate a weak sphincter, which could be contributing to her symptoms.\n\nNow, considering the findings from these tests, what would cystometry reveal? Cystometry is a diagnostic tool that examines how the bladder and urethral sphincter function during filling and emptying. It can show things like how much the bladder can hold (residual volume) and how well the sphincter closes during contractions.\n\nGiven her history and the Q-tip results, if the sphincter is weak, the detrusor contractions (the contractions of the bladder muscle) might not be sufficient to prevent leakage. So, during cystometry, the residual volume might be normal or slightly increased because the sphincter can't hold the usual amount. Additionally, the detrusor contractions might be weak or ineffective, leading to involuntary leakage when the bladder pressure exceeds the sphincter's closure pressure.\n\nI think the key points are that residual volume might be slightly elevated due to the sphincter's weakness, and detrusor contractions would be insufficient to prevent leakage. So, the cystometry would likely show an increased residual volume and weak or ineffective detrusor contractions.\n</think>\n\nCystometry for this 61-year-old woman would most likely reveal an increased residual volume and weak or ineffective detrusor contractions. The findings suggest a weak urethral sphincter contributing to her stress urinary incontinence, characterized by insufficient sphincter closure during activities like coughing.<ï½œendâ–ofâ–sentenceï½œ>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#Update the system prompt to show the internal thinking inside Chain of thought COT \ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:46:58.905875Z","iopub.execute_input":"2025-02-14T13:46:58.906176Z","iopub.status.idle":"2025-02-14T13:46:58.911486Z","shell.execute_reply.started":"2025-02-14T13:46:58.906153Z","shell.execute_reply":"2025-02-14T13:46:58.910560Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Load dataset https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT \n#from datasets import load_dataset \ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[0:500]\", trust_remote_code=True)\n#dataset = dataset.map(formatting_prompts_func, batched = True)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:47:09.474167Z","iopub.execute_input":"2025-02-14T13:47:09.474482Z","iopub.status.idle":"2025-02-14T13:47:12.716389Z","shell.execute_reply.started":"2025-02-14T13:47:09.474454Z","shell.execute_reply":"2025-02-14T13:47:12.715686Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e09cfebf5274c8b822d74a416bd6699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f767544f9c44a1c84e7d0baf4bf2b9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25371 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b3f970f3904e93949c1a7e393ba6ce"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Show an entry from the dataset\ndataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:47:21.401209Z","iopub.execute_input":"2025-02-14T13:47:21.401608Z","iopub.status.idle":"2025-02-14T13:47:21.412862Z","shell.execute_reply.started":"2025-02-14T13:47:21.401573Z","shell.execute_reply":"2025-02-14T13:47:21.410980Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'Question': 'A 45-year-old man with a history of alcohol use, who has been abstinent for the past 10 years, presents with sudden onset dysarthria, shuffling gait, and intention tremors. Given this clinical presentation and history, what is the most likely diagnosis?',\n 'Complex_CoT': \"Alright, letâ€™s break this down. We have a 45-year-old man here, who suddenly starts showing some pretty specific symptoms: dysarthria, shuffling gait, and those intention tremors. This suggests something's going wrong with motor control, probably involving the cerebellum or its connections.\\n\\nNow, what's intriguing is that he's had a history of alcohol use, but he's been off it for the past 10 years. Alcohol can do a number on the cerebellum, leading to degeneration, and apparently, the effects can hang around or even appear long after one stops drinking.\\n\\nAt first glance, these symptoms look like they could be some kind of chronic degeneration, maybe something like alcoholic cerebellar degeneration, but hold on. This looks different. The symptoms just came on all of a sudden. Chronic degenerations typically have a more gradual onset.\\n\\nOkay, letâ€™s reconsider this sudden nature. Itâ€™s making me think of something more acute, more rapid onset. Hmm, if we dig back into his history of drinking, there might have been some damage done, leading to nutritional deficiencies, like a lack of thiamine.\\n\\nWernickeâ€™s encephalopathy is a classic possibility here. That condition pops up due to thiamine deficiency, often after a history of alcohol use. Itâ€™s known for causing ataxia, confusion, and eye movement issues. However, heâ€™s not showing the full triad of symptoms; there's no mention of confusion or eye problems, so maybe it doesnâ€™t fit perfectly.\\n\\nOh, wait a second, maybe we're missing something simpler. Given the acute nature of the symptoms, maybe this is more in line with something like a stroke. Sudden onset can definitely suggest a vascular event. With his alcohol history, heâ€™s at risk for things like high blood pressure, which can increase stroke risk.\\n\\nIn particular, lacunar strokes can mess with motor coordination, speech, the works. These are small, but significant enough to cause these exact symptoms: dysarthria, tremors, and a shuffling gait.\\n\\nBut hang on, what if thereâ€™s another angle weâ€™re missing? Chronic alcohol effects might still play a role here, just in a different way. Thereâ€™s a condition called acquired hepatocerebral degeneration. This can happen in people with a history of liver disease due to alcohol, even years after theyâ€™ve quit. It can result in symptoms like these because of how substances get deposited in the brain.\\n\\nLinking this back to our patient, his history with alcohol couldâ€™ve led to some liver issues, and now, here we are with these symptoms showing up suddenly. Makes sense, right? So, considering everything, acquired hepatocerebral degeneration seems to fit quite nicely. Thatâ€™s probably our most fitting diagnosis for this situation.\",\n 'Response': \"Considering the clinical presentation of sudden onset dysarthria, shuffling gait, and intention tremors in a 45-year-old man with a history of alcohol use who has been abstinent for the past 10 years, the most likely diagnosis is acquired hepatocerebral degeneration.\\n\\nThis condition is associated with chronic liver disease, which can often be a consequence of long-term alcohol use. Despite the patient's abstinence from alcohol for a decade, previous alcohol use may have led to underlying liver dysfunction. This dysfunction, even if subclinical, can cause encephalopathy due to the accumulation of neurotoxic substances that affect the brain. The sudden onset of these neurological symptoms aligns with how acquired hepatocerebral degeneration can manifest, making it a probable diagnosis in this scenario.\"}"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# We need to format the dataset to fit our prompt training style \nEOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which will define the end of sentence ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:51:02.347567Z","iopub.execute_input":"2025-02-14T13:51:02.347946Z","iopub.status.idle":"2025-02-14T13:51:02.353329Z","shell.execute_reply.started":"2025-02-14T13:51:02.347919Z","shell.execute_reply":"2025-02-14T13:51:02.352557Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#Function that formats the prompt\ndef formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]           #Extracts the medical question from the dataset\n    cots = examples[\"Complex_CoT\"]          #Extracts the chain-of-thought from the dataset\n    outputs = examples[\"Response\"]          #Extracts the model generated response \n    \n    texts = []                              #Initializes an empty list to store the formatted prompts \n    \n    for input, cot, output in zip(inputs, cots, outputs):\n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # Insert values into prompt template & append EOS token\n        texts.append(text)                 # Add the formatted text to the list\n    return {\n        \"text\": texts                      # Return the newly formatted dataset with a \"text\" column containing structured prompts\n    }\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:51:19.065670Z","iopub.execute_input":"2025-02-14T13:51:19.065998Z","iopub.status.idle":"2025-02-14T13:51:19.071360Z","shell.execute_reply.started":"2025-02-14T13:51:19.065962Z","shell.execute_reply":"2025-02-14T13:51:19.070734Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset_finetune = dataset.map(formatting_prompts_func, batched = True)\ndataset_finetune[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:05:58.287144Z","iopub.execute_input":"2025-02-14T14:05:58.287453Z","iopub.status.idle":"2025-02-14T14:05:58.335829Z","shell.execute_reply.started":"2025-02-14T14:05:58.287431Z","shell.execute_reply":"2025-02-14T14:05:58.335271Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cd084b785474d66a891924d1a5101e9"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<ï½œendâ–ofâ–sentenceï½œ>\""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"#Prepare model properly for training before wrapping it with LoRA and Ensures generate() is set correctly so Unsloth doesnâ€™t break it later.\nFastLanguageModel.for_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:06:26.086286Z","iopub.execute_input":"2025-02-14T14:06:26.086567Z","iopub.status.idle":"2025-02-14T14:06:26.095877Z","shell.execute_reply.started":"2025-02-14T14:06:26.086545Z","shell.execute_reply":"2025-02-14T14:06:26.095052Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n    (layers): ModuleList(\n      (0): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n      (1): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n      (2-31): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#Setup the model using Low Rank Adaptation - LoRA\n#ğŸ”¥ ğŸ’¡ Wraps an LLM with LoRA for memory-efficient fine-tuning ğŸ’¡ Only updates key transformer layers (q_proj, k_proj, etc.) ğŸ’¡ Uses Unslothâ€™s optimizations (low memory, faster training) ğŸ’¡ Enables gradient checkpointing to train longer-context models\n# wrap the model with LoRA using Unsloth's methods and optimized LoRA implementation. get_peft_model() is a function which stands for Parameter-Efficient Fine-Tuning â€” this function wraps the base model (model) with LoRA modifications, ensuring that only specific parameters are trained.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16, # increase r for better quality \n    target_modules=[ #modules of transformer layer\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16, #Scaling factor for lora updates. higher lora = stronger finetune impact. Value of 16 is balanced. \n    lora_dropout=0, #dropout rate during lora training. 0 means no dropout fully training lora layers *** touch to avoid overfitting e.g. 0.1 \n    bias=\"none\", #tells lora not to modify the bias terms in the model \n    use_gradient_checkpointing=\"unsloth\", #additional reduction of memory usage (more optimized than standard HF)\n    random_state=3407, #Set a random seed for reproduceability \n    use_rslora=False, #disable rslora rank stabilized lora \n    loftq_config=None, #LoFTQ (Low-Frequency Quantization) is another optimization for extreme low memory finetuning     \n)\n\n#32 Transformer Layers Patched\n#âœ” QKV Layers: 32\n#âœ” O Layers: 32\n#âœ” MLP Layers: 32\n#LoRA has successfully wrapped the DeepSeek-R1 model!\n#Fine-tuning setup is fully optimized and memory-efficient.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:06:36.691395Z","iopub.execute_input":"2025-02-14T14:06:36.691738Z","iopub.status.idle":"2025-02-14T14:06:43.313595Z","shell.execute_reply.started":"2025-02-14T14:06:36.691706Z","shell.execute_reply":"2025-02-14T14:06:43.312917Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#- âœ… SFTTrainer â†’ A trainer for supervised fine-tuning (SFT) of LLMs\n#- âœ… TrainingArguments â†’ Defines all fine-tuning hyperparameters\n#- âœ… is_bfloat16_supported() â†’ Checks if the GPU supports bfloat16 (important for mixed precision training)\n#Set up the model \n\n#from trl import SFTTrainer #Hugging Face's Transformers Reinforcement Learning library\n#from transformers import TrainingArguments\n#from unsloth import is_bfloat16_supported \n\ntrainer = SFTTrainer(\n    model=model, #the LLM to finetune \n    tokenizer=tokenizer, #tokenizer for text processing  \n    train_dataset=dataset_finetune, # the dataset for finetuning \n    dataset_text_field=\"text\", #field in dataset containing text data \n    max_seq_length=max_seq_length, #max token sequence length\n    dataset_num_proc=2, #Number of parallel processes for dataset loading\n\n    #define training arguments \n    args=TrainingArguments(\n        per_device_train_batch_size=2, #trains 2 samples per GPU Batch - increase for faster training \n        gradient_accumulation_steps=4, #accumulates gradients over 4 steps before updating the model\n        num_train_epochs = 1, # commment and set warmup_ratio for full training runs!\n        warmup_steps=5, #gradually increases learning rate over 5 steps and prevents the model from making drastic steps too early \n        max_steps=60, #fine tuning stops after 60 steps. If doing a full fine tune use num_train_epochs = 1 instead. \n        learning_rate=2e-4, #0.0002 how fast the model updates weights. higher LR faster training with risk of instability. Lower LR slower training but more stable finetuning \n        fp16=not is_bfloat16_supported(), #uses bfloat16 if the GPU supports it ...\n        bf16=is_bfloat16_supported(), #...otherwise falls back to fp16 (16 bit precision)\n        logging_steps=10, #logs training metrics every 10 steps for training performance \n        optim=\"adamw_8bit\", #Uses Adamw optimizer but in 8 bit mode thus saving VRAM and speeding up training \n        weight_decay=0.01, #Adds regularization to prevent overfitting \n        lr_scheduler_type=\"linear\", #uses a linear rate schedule (gradually decreases LR) to keep training stable without sudden jumps\n        seed=3407, #a random seet to ensure consistency across runs\n        output_dir=\"outputs\", #saves the finetuned model checkpoints and logs inside the outputs directory \n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:07:02.333711Z","iopub.execute_input":"2025-02-14T14:07:02.334023Z","iopub.status.idle":"2025-02-14T14:07:05.194122Z","shell.execute_reply.started":"2025-02-14T14:07:02.334001Z","shell.execute_reply":"2025-02-14T14:07:05.193164Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540aa6c5f3404143825d27c008c5c874"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"#Supplimentary commands to solve issues for reference \ntorch.cuda.memory_summary(device=\"cuda\")\nprint(hasattr(model, \"_unwrapped_old_generate\"))\n\nif hasattr(model, \"_unwrapped_old_generate\"):\n    print(\"Type:\", type(model._unwrapped_old_generate))\n    print(\"Function Code:\", model._unwrapped_old_generate.__code__)\n\nif hasattr(model, \"_unwrapped_old_generate\"):\n    print(\"Same as generate():\", model._unwrapped_old_generate == model.generate)\n\nif hasattr(model, \"_unwrapped_old_generate\"):\n    model._unwrapped_old_generate = None  # Since unwrapped and generate are identical, lets detach this because it is a bug in UNsloth - Just nullify it instead of deleting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Start fine-tuning \ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:07:21.507052Z","iopub.execute_input":"2025-02-14T14:07:21.507419Z","iopub.status.idle":"2025-02-14T14:27:04.797683Z","shell.execute_reply.started":"2025-02-14T14:07:21.507389Z","shell.execute_reply":"2025-02-14T14:27:04.797035Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 19:13, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.918900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.461500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.402300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.308800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.344300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.314000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#Save the fine-tuned model to wandb\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:27:55.078195Z","iopub.execute_input":"2025-02-14T14:27:55.078503Z","iopub.status.idle":"2025-02-14T14:27:56.549455Z","shell.execute_reply.started":"2025-02-14T14:27:55.078479Z","shell.execute_reply":"2025-02-14T14:27:56.548842Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>â–â–‚â–„â–…â–‡â–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–‚â–„â–…â–‡â–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–‚â–‚â–â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–‡â–…â–„â–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–ƒâ–‚â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.8014312853602304e+16</td></tr><tr><td>train/epoch</td><td>0.96</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.26023</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.314</td></tr><tr><td>train_loss</td><td>1.4583</td></tr><tr><td>train_runtime</td><td>1181.0787</td></tr><tr><td>train_samples_per_second</td><td>0.406</td></tr><tr><td>train_steps_per_second</td><td>0.051</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">thrilling-caress-7</strong> at: <a href='https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/i1hlllxs' target=\"_blank\">https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/i1hlllxs</a><br> View project at: <a href='https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset' target=\"_blank\">https://wandb.ai/nikolaid-3cx/fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250214_134522-i1hlllxs/logs</code>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"#run model inference after fine-tuning\n\n#Test Deepseek by providing a medical question and generating a response \n# Set a test question \n# Format the question using structured Prompt style, Tokenize the input and  move it to GPU \n# Generate a response \n# Decode the output tokens back into the text to get the final readable response \n\nquestion = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\nFastLanguageModel.for_inference(model) #Unsloth is x 2 faster than huggingface \n\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  #tokenized input questions moved to the gpu \n\n# get more param ideas from the cat to tweak the generation settings to improve responses \noutputs = model.generate(\n    input_ids=inputs.input_ids,               \n    attention_mask=inputs.attention_mask,     \n    max_new_tokens=1200,                       \n    use_cache=True,\n)\n\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:31:46.251179Z","iopub.execute_input":"2025-02-14T14:31:46.251497Z","iopub.status.idle":"2025-02-14T14:32:15.080645Z","shell.execute_reply.started":"2025-02-14T14:31:46.251471Z","shell.execute_reply":"2025-02-14T14:32:15.079949Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so let's think about this. We have a 61-year-old woman who's been dealing with involuntary urine loss during things like coughing or sneezing, but she's not leaking at night. That suggests she might have some kind of problem with her pelvic floor muscles or maybe her bladder.\n\nNow, she's got a gynecological exam and a Q-tip test. Let's break that down. The Q-tip test is usually used to check for urethral obstruction. If it's positive, that means there's something blocking the urethra, like a urethral stricture or something else.\n\nGiven that she's had a positive Q-tip test, it's likely there's a urethral obstruction. That would mean her urethra is narrow, maybe due to a stricture or some kind of narrowing. So, her bladder can't empty properly during activities like coughing because the urethral obstruction is making it hard.\n\nNow, let's think about what happens when her bladder can't empty. If there's a urethral obstruction, the bladder is forced to hold more urine, increasing the residual volume. That's because her bladder doesn't empty completely. So, her residual volume is probably increased.\n\nAlso, if her bladder can't empty properly, she might have increased detrusor contractions. These contractions are usually stronger to push the urine out. So, we expect her detrusor contractions to be increased.\n\nPutting it all together, if she has a urethral obstruction and a positive Q-tip test, we'd expect her cystometry results to show increased residual volume and increased detrusor contractions. That makes sense because of the obstruction and how her bladder is trying to compensate by contracting more.\n</think>\nBased on the findings of the gynecological exam and the positive Q-tip test, it is most likely that the cystometry would reveal increased residual volume and increased detrusor contractions. The positive Q-tip test indicates urethral obstruction, which would force the bladder to retain more urine, increasing the residual volume. Additionally, the detrusor contractions may be increased as the bladder compensates for the obstruction, attempting to push more urine out despite the obstruction.<ï½œendâ–ofâ–sentenceï½œ>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"question = \"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. What is the most likely predisposing factor for this patient's condition?\"\n\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\n\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:33:14.374269Z","iopub.execute_input":"2025-02-14T14:33:14.374562Z","iopub.status.idle":"2025-02-14T14:34:04.129551Z","shell.execute_reply.started":"2025-02-14T14:33:14.374539Z","shell.execute_reply":"2025-02-14T14:34:04.128745Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nAlright, let's think about this. We've got a 59-year-old guy who's got a fever, chills, night sweats, and feels pretty tired. That sounds like a classic picture of an infection, right? And there's a vegetation on his aortic valve. Hmm, that's not great news. \n\nNow, looking at the blood culture results: gram-positive, catalase-negative, gamma-hemolytic cocci in chains. That means it's a Streptococcus species, specifically Streptococcus pyogenes. It's not Streptococcus pneumoniae because it doesn't grow in NaCl. And this guy's symptoms, combined with the vegetation, point towards endocarditis, which is an infection of the heart valves.\n\nOkay, so let's figure out what could have caused this. I'm thinking about infections, not just from bacteria, but also from fungi or viruses. It's important to consider these possibilities because sometimes you can miss something. \n\nBut wait, let's think about the main culprit. Streptococcus pyogenes is pretty aggressive and can cause a lot of problems, especially if it gets into the bloodstream. That can lead to endocarditis if it takes hold in the heart. Now, why would this happen?\n\nWell, there are a few things that can increase the risk. One big factor is the presence of an underlying condition that weakens the immune system. That could be something like diabetes, heart disease, or even something like chronic kidney disease. All these things can make it harder for the body to fight off infections effectively.\n\nAnother thought is about medical procedures. Sometimes, when you have a medical procedure or device, like a pacemaker or a catheter, there's a risk of infection if it's not properly sterilized or if it's left in place for too long. This is called Nosocomial infection.\n\nOh, and there's also the possibility of it being due to the use of steroids or other medications. These can suppress the immune system and make it easier for an infection to take hold. \n\nHmm, let's think about these factors. We've got diabetes, heart disease, and medical procedures. These are all possible reasons why Streptococcus pyogenes could be causing this. \n\nBut let's not forget that Streptococcus pyogenes itself can be quite virulent. It doesn't need a weakened immune system to cause an infection. It can just thrive in the body naturally. So, it's not just about the environment, but also about the nature of the bacteria itself. \n\nAlright, so to sum up, the most likely causes for this infection could be due to a weakened immune system, like from diabetes or heart disease, or it could be from medical procedures that aren't properly kept sterile, or even from using medications that suppress the immune system. \n\nIn this case, given that it's a gram-positive, catalase-negative, gamma-hemolytic cocci in chains, it's definitely Streptococcus pyogenes. So, we need to focus on what could have made this infection take hold in the heart. \n\nConsidering everything, I think the most likely predisposing factor here is that the patient had an underlying condition that made his immune system weaker, such as diabetes or heart disease. This would have made it easier for the bacteria to cause an infection like endocarditis. \n\nSo, yeah, I'm pretty confident that underlying health conditions, like diabetes or heart disease, are the main contributors here. That makes sense given the symptoms and the type of bacteria involved.\n</think>\nThe most likely predisposing factor for this patient's condition is an underlying health condition that weakens the immune system, such as diabetes or heart disease. These conditions can make it easier for Streptococcus pyogenes to thrive and cause an infection like endocarditis.<ï½œendâ–ofâ–sentenceï½œ>\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#Save the model locally \nnew_model_online = \"develops20/DeepSeek-R1-Distill-Llama-8B-Medical-COT\"\nnew_model_local = \"DeepSeek-R1-Distill-Llama-8B-Medical-COT\"\nmodel.save_pretrained(new_model_local) #local saved model \ntokenizer.save_pretrained(new_model_local)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:36:36.680265Z","iopub.execute_input":"2025-02-14T14:36:36.680585Z","iopub.status.idle":"2025-02-14T14:36:37.925983Z","shell.execute_reply.started":"2025-02-14T14:36:36.680561Z","shell.execute_reply":"2025-02-14T14:36:37.925042Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('DeepSeek-R1-Distill-Llama-8B-Medical-COT/tokenizer_config.json',\n 'DeepSeek-R1-Distill-Llama-8B-Medical-COT/special_tokens_map.json',\n 'DeepSeek-R1-Distill-Llama-8B-Medical-COT/tokenizer.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"#Push the model to hugging face hub \nmodel.push_to_hub(new_model_online) #save online and pushed to hub\ntokenizer.push_to_hub(new_model_online) #save online and pushed to hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:40:57.560605Z","iopub.execute_input":"2025-02-14T14:40:57.561066Z","iopub.status.idle":"2025-02-14T14:41:01.875979Z","shell.execute_reply.started":"2025-02-14T14:40:57.561023Z","shell.execute_reply":"2025-02-14T14:41:01.875282Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ab6e9a23d041e9b2e59aa4755f7c52"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Saved model to https://huggingface.co/develops20/DeepSeek-R1-Distill-Llama-8B-Medical-COT\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d98d4184ba497fad83352cf446c868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30547a62514842c2b090479f1ef4d4b7"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"#âœ… This saves the fine-tuned model to a local directory (new_model_local).\n#âœ… Merges the LoRA adapters into the base model.\n#âœ… Stores the model in 16-bit precision (merged_16bit) to save space. other options are merged_32bit\nmodel.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\", )\n\n#âœ… Uploads the fine-tuned model to Hugging Face Hub (new_model_online).\n#âœ… Merges LoRA layers into the base model before uploading.\n#âœ… Uses \"merged_16bit\" to keep file size reasonable.\n#âœ… Allows public or private access for inference and sharing.\n\nmodel.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:43:12.234841Z","iopub.execute_input":"2025-02-14T14:43:12.235181Z","iopub.status.idle":"2025-02-14T14:49:59.251945Z","shell.execute_reply.started":"2025-02-14T14:43:12.235141Z","shell.execute_reply":"2025-02-14T14:49:59.251003Z"}},"outputs":[{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 6.0G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.17 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":" 34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:00<00:01, 15.67it/s]\nWe will save to Disk and not RAM now.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:25<00:00,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00004-of-00004.bin...\nDone.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: You are pushing to hub in Kaggle environment.\nTo save memory, we shall move develops20/DeepSeek-R1-Distill-Llama-8B-Medical-COT to /tmp/DeepSeek-R1-Distill-Llama-8B-Medical-COT\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.12 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:25<00:00,  1.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer...","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":" Done.\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-Medical-COT/pytorch_model-00004-of-00004.bin...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36d6fc93975544cb872ab366eff5ce97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771858cec1a049458891b138429278cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c8af3cd9404a87b3841597ca9e17c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db698bbdf04e470fb48b4266ba544c25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d598813069ca420fb3b2279b1a472ba1"}},"metadata":{}},{"name":"stdout","text":"Done.\nSaved merged model to https://huggingface.co/develops20/DeepSeek-R1-Distill-Llama-8B-Medical-COT\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}